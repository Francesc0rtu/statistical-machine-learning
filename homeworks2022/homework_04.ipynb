{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Let $\\theta_1$ and $\\theta_2$ be real valued parameters in $[0,1]$ and consider the generative model\n",
    "\\begin{align*}\n",
    "\\theta_1 &\\sim \\theta_1\\text{-prior}\\\\\n",
    "\\theta_2 &\\sim \\theta_2\\text{-prior}\\\\\n",
    "\\hat{y} &= \\frac{\\theta_1+x^2}{\\theta_2\\cdot x}\\\\\n",
    "y &\\sim \\mathcal{N} (\\hat{y}, 1)\n",
    "\\end{align*}\n",
    "\n",
    "a. Use pyro to implement the model as a function `model(theta1_prior, theta2_prior, x, obs)`, where `theta1_prior` and `theta2_prior` are pyro.distributions objects, `x` and `obs` are torch tensors, and draws from the normal distribution are conditioned on `obs`.\n",
    "\n",
    "b. Choose two suitable prior distributions for $\\theta_1$ and $\\theta_2$ (e.g. suitably rescaled Normal or Beta distributions)  and use HMC or NUTS algorithm to find their posterior distributions given the observations\n",
    "\n",
    "\\begin{align*}\n",
    "x&=(47,87,20,16,38,5)\\\\\n",
    "y&=(58.76, 108.75,  25.03,  20.03,  47.51,  6.37).\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "c. Discuss how different prior distributions lead to different estimates of $\\theta_1$ and $\\theta_2$. Comment on the convergence checks and plot the posterior distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer.mcmc import MCMC, NUTS\n",
    "\n",
    "def model(theta1_prior, theta2_prior, x, obs):\n",
    "    theta_1 = pyro.sample('theta_1', theta1_prior)\n",
    "    theta_2 = pyro.sample('theta_2', theta2_prior)\n",
    "    y = (theta_1+x**2)/(theta_2*x)\n",
    "    return pyro.sample('y', dist.Normal(y,1), obs=obs)\n",
    "\n",
    "x = torch.tensor([47,87,20,16,38,5])\n",
    "y = torch.tensor([26.1407, 48.3493, 11.1806,  8.9757, 21.1477,  3.0556])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 6000/6000 [00:54, 110.30it/s, step size=5.50e-01, acc. prob=0.901]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "   theta_1      0.76      0.18      0.81      0.48      1.00    521.79      1.00\n",
      "   theta_2      1.00      0.00      1.00      1.00      1.00    758.63      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nuts_kernel = NUTS(model)\n",
    "mcmc = MCMC(nuts_kernel, warmup_steps=5000, num_samples=1000, num_chains=1)\n",
    "mcmc.run(theta1_prior=dist.Beta(5,1), theta2_prior=dist.Beta(2,0.5), x=x, obs=y)\n",
    "mcmc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "A bivariate Gibbs sampler for a vector $x=(x_1,x_2)$ draws iteratively from the posterior conditional distributions in the following way:\n",
    "- choose a starting value $p(x_1|x_2^{(0)})$\n",
    "- for each iteration $i$:\n",
    "    - draw $x_2(i)$ from $p(x_2|x_1^{(i-1)})$\n",
    "    - draw $x_1(i)$ from $p(x_1|x_2^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Supposing that samples are drawn from a bivariate normal distribution\n",
    "\n",
    "$$\n",
    "{x_1 \\choose x_2} \\sim \\mathcal{N} \\Bigg[ {0 \\choose 0} , \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix} \\Bigg],\n",
    "$$\n",
    "    implement a Gibbs sampler function which takes as inputs the parameter `rho`, the number of iterations `iters` and the number of warmup draws `warmup`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_sampler(x1_init, x2_init, rho, iters, warmup):\n",
    "    x1 = torch.zeros(warmup+iters)\n",
    "    x2 = torch.zeros(warmup+iters)\n",
    "    \n",
    "    x1[0] = x1_init\n",
    "    x2[0] = x2_init\n",
    "    \n",
    "    for i in range(1, warmup+iters):\n",
    "        #x1[i] = pyro.sample('x1', dist.Normal(rho*x2[i-1].item(), np.sqrt(1-rho**2)))\n",
    "        #x2[i] = pyro.sample('x2', dist.Normal(rho*x1[i-1].item(), np.sqrt(1-rho**2)))\n",
    "        x1[i] = rho*x2[i-1].item()+np.sqrt(1-rho**2)*pyro.sample('x1', dist.Normal(0, 1))\n",
    "        x2[i] = rho*x1[i-1].item()+np.sqrt(1-rho**2)*pyro.sample('x2', dist.Normal(0, 1))\n",
    "        \n",
    "    x1 = x1[warmup:]\n",
    "    x2 = x2[warmup:]\n",
    "    \n",
    "    return x1, x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Use your implementation of Gibbs sampler to infer the parameters $\\theta=(\\theta_1,\\theta_2)$ from **Exercise 1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.042326237284318134"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta1_samples = mcmc.get_samples()['theta_1']\n",
    "theta2_samples = mcmc.get_samples()['theta_2']\n",
    "sigma = np.corrcoef([theta1_samples.numpy(), theta2_samples.numpy()])\n",
    "estimated_rho = sigma[0,1]\n",
    "estimated_rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0199) tensor(-0.0104)\n"
     ]
    }
   ],
   "source": [
    "theta1, theta2 = gibbs_sampler(x1_init=theta1_samples.mean(), x2_init=theta2_samples.mean(), rho=estimated_rho, \n",
    "                               iters=1000, warmup=10000)\n",
    "\n",
    "print(theta1.mean(), theta2.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7639) tensor(0.9999)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def conditional_sampler(sampling_index, current_x, mean, cov):\n",
    "    conditioned_index = 1 - sampling_index \n",
    "    # The above line works because we only have 2 variables, x_0 & x_1\n",
    "    a = cov[sampling_index, sampling_index]\n",
    "    b = cov[sampling_index, conditioned_index]\n",
    "    c = cov[conditioned_index, conditioned_index]\n",
    "\n",
    "    mu = mean[sampling_index] + (b * (current_x[conditioned_index] - mean[conditioned_index]))/c\n",
    "    sigma = np.sqrt(a-(b**2)/c)\n",
    "    new_x = np.copy(current_x)\n",
    "    new_x[sampling_index] = np.random.randn()*sigma + mu\n",
    "    return new_x\n",
    "\n",
    "def gibbs_sampler(initial_point, num_samples, mean, cov):\n",
    "\n",
    "    point = np.array(initial_point)\n",
    "    samples = np.empty([num_samples+1, 2])  #sampled points\n",
    "    samples[0] = point\n",
    "    tmp_points = np.empty([num_samples, 2]) #inbetween points\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Sample from p(x_0|x_1)\n",
    "        point = conditional_sampler(0, point, mean, cov)\n",
    "        tmp_points[i] = point\n",
    "        # Sample from p(x_1|x_0)\n",
    "        point = conditional_sampler(1, point, mean, cov)\n",
    "        samples[i+1] = point\n",
    "\n",
    "    return samples, tmp_points\n",
    "\n",
    "print(theta1_samples.mean(), theta2_samples.mean())\n",
    "\n",
    "theta1, theta2 = gibbs_sampler(initial_point=[theta1_samples.mean(), theta2_samples.mean()], \n",
    "                               cov=sigma, num_samples=10000, mean=[theta1_samples.mean(), theta2_samples.mean()])\n",
    "\n",
    "print(theta1.mean(), theta2.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
