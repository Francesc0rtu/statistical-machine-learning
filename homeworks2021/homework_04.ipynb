{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 \n",
    "\n",
    "A bivariate Gibbs sampler for a vector $x=(x_1,x_2)$ draws iteratively from the conditional distributions in the following way:\n",
    "- choose a starting value $p(x_1|x_2^{(0)})$\n",
    "- for each iteration $i$:\n",
    "    - draw $x_2^{(i)}$ from $p(x_2|x_1^{(i-1)})$\n",
    "    - draw $x_1^{(i)}$ from $p(x_1|x_2^{(i)})$\n",
    "    \n",
    "As in other MCMC algorithms, samples from the first few iterations are usually discarded. This is known as the \"warmup\" phase. \n",
    "\n",
    "Suppose that the conditional distributions are\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_1 | x_2 &\\sim \\mathcal{N}\\Big( \\mu_1+\\rho\\frac{\\sigma_1}{\\sigma_2}(x_2-\\mu_2), \\sigma_1^2(1-\\rho)\\Big) \\\\\n",
    "x_2 | x_1 &\\sim \\mathcal{N}\\Big( \\mu_2+\\rho\\frac{\\sigma_2}{\\sigma_1}(x_1-\\mu_1), \\sigma_2^2(1-\\rho)\\Big)\n",
    "\\end{align*}\n",
    ",\n",
    "$$\n",
    "\n",
    "where $\\rho, \\mu_1, \\mu_2, \\sigma_1, \\sigma_2$ are real valued parameters and $|\\rho|<1$.\n",
    "\n",
    "1. (**code**) Implement a Gibbs sampler which takes as inputs the number of warmup draws `warmup`, the number of iterations `iters` and the parameters `rho, mu1, mu2, sigma1, sigma2`.\n",
    "\n",
    "2. (**code**) Set $\\rho=0.3$, $\\mu_1=0.5$, $\\mu_2=0.1$, $\\sigma_1=1$, $\\sigma_2=1.5$ and plot the distributions of $x_1$ and $x_2$. \n",
    "\n",
    "3. (**theory**) What is the joint distribution of $x=(x_1, x_2)$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "(**code**)\n",
    "\n",
    "Let $\\theta_1$ and $\\theta_2$ be real valued parameters of the function \n",
    "\n",
    "$$f(x, \\theta_1, \\theta_2)=\\frac{e^{\\theta_1 x}+\\theta_2}{x}.$$\n",
    "\n",
    "We observe some noisy realizations $(x_{obs}, y_{obs})$ of this function and want to infer the posterior distribution of $\\theta_1$ and $\\theta_2$.\n",
    "\n",
    "\n",
    "1. Generate $30$ observations $(x_{obs}, y_{obs})$ as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_1 &= 0.5 \\\\\n",
    "\\theta_2 &= 3 \\\\\n",
    "\\rho &\\sim \\mathcal{N}(0,0.3^2)\\\\\n",
    "x_{obs} &\\sim \\mathcal{U}(0,10)\\\\\n",
    "y_{obs} &= f(x_{obs}, \\theta_1, \\theta_2) + \\rho\n",
    "\\end{align*}\n",
    "\n",
    "and plot them using seaborn `sns.scatterplot` function.\n",
    "\n",
    "2. Given the observations from (1.) and the generative model\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_1 &\\sim \\text{Exp}(2.)\\\\\n",
    "\\theta_2 &\\sim \\text{Exp}(0.2)\\\\\n",
    "\\gamma &\\sim \\mathcal{U}(0, 0.5)\\\\\n",
    "\\hat{y} &= f(x, \\theta_1, \\theta_2)\\\\\n",
    "y & \\sim \\mathcal{N}(\\hat{y},\\gamma)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "use pyro `NUTS` to infer the posterior distribution of $\\theta_1, \\theta_2, \\gamma$. Set `warmup_steps=2000` and `num_samples=100`.\n",
    "\n",
    "3. Look at convergence checks on the traces and discuss the quality of your estimates. Remeber that you can extract posterior samples from an `MCMC` object using `get_samples()` method (notebook 04).\n",
    "\n",
    "\n",
    "4. Use `sns.scatterplot` to plot the observed couples $(x_{obs}, y_{obs})$ and `sns.lineplot` to plot the learned function $f$ with the posterior estimates  $\\theta_1, \\theta_2$. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
