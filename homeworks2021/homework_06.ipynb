{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 06\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "import sklearn.gaussian_process as gp_sklearn\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import MCMC, HMC, NUTS\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO\n",
    "from pyro.contrib.autoguide import AutoDiagonalNormal\n",
    "from pyro.optim import Adam\n",
    "import pyro.contrib.gp as gp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a binary classification problem on Half Moons dataset, which consists of two interleaving half circles. The input is two-dimensional and the response is binary (0,1).\n",
    "\n",
    "We observe $100$ points $x$ from this dataset and their labels $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_moons(n_samples=100, shuffle=True, noise=0.1, random_state=1)\n",
    "x = torch.from_numpy(x)\n",
    "y = torch.from_numpy(y).double()\n",
    "\n",
    "def scatterplot(x, y):\n",
    "    colors = np.array(['0', '1'])\n",
    "    sns.scatterplot(x[:, 0], x[:, 1], hue=colors[y.int()])\n",
    "    \n",
    "scatterplot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit-learn GaussianProcessClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `GaussianProcessClassifier` from scikit-learn library [1] approximates the non-Gaussian posterior by a Gaussian using Laplace approximation. Define an RBF kernel `gp_sklearn.kernels.RBF` with lenghtscale parameter $=1$ and fit a Gaussian Process classifier to the observed data (x,y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use `plot_sklearn_predictions` function defined below to plot the posterior predictive mean function over a finite grid of points. You should pass as inputs the learned GP classifier `sklearn_gp_classifier`, the observed points `x` and their labels `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meshgrid(x, n, eps=0.1):\n",
    "    x0, x1 = np.meshgrid(np.linspace(x[:, 0].min()-eps, x[:, 0].max()+eps, n),\n",
    "                         np.linspace(x[:, 1].min()-eps, x[:, 1].max()+eps, n))\n",
    "    x_grid = np.stack([x0.ravel(), x1.ravel()], axis=-1)\n",
    "    return x0, x1, x_grid\n",
    "\n",
    "def plot_sklearn_predictions(sklearn_gp_classifier, x, y):\n",
    "    x0, x1, x_grid = meshgrid(x, 30) \n",
    "\n",
    "    preds = sklearn_gp_classifier.predict_proba(x_grid)\n",
    "    preds_0 = preds[:,0].reshape(x0.shape)\n",
    "    preds_1 = preds[:,1].reshape(x0.shape)\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.contourf(x0, x1, preds_0, 101, cmap=plt.get_cmap('bwr'), vmin=0, vmax=1)\n",
    "    plt.contourf(x0, x1, preds_1, 101, cmap=plt.get_cmap('bwr'), vmin=0, vmax=1)\n",
    "\n",
    "    plt.title(f'Posterior Mean')\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "    plt.colorbar()\n",
    "    scatterplot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyro classification with HMC inference \n",
    "Consider the following generative model\n",
    "\n",
    "\\begin{align*}\n",
    "y_n| p_n &\\sim \\text{Bernoulli}(p_n) \\qquad n=1,\\ldots,N\\\\\n",
    "\\text{logit}(\\mathbf{p})|\\mu,\\sigma,l &\\sim \\mathcal{GP}(\\mu, K_{\\sigma,l}(x_n))\\\\\n",
    "\\mu &\\sim \\mathcal{N}(0,1)\\\\\n",
    "\\sigma &\\sim \\text{LogNormal}(0,1)\\\\\n",
    "l &\\sim \\text{LogNormal}(0,1)\n",
    "\\end{align*}\n",
    "\n",
    "We model the binary response variable with a Bernoulli likelihood. The logit of the probability is a Gaussian Process with predictors $x_n$ and kernel matrix $K_{\\sigma,l}$, parametrized by variance $\\rho$ and lengthscale $l$.\n",
    "\n",
    "We want to solve this binary classification problem by means of HMC inference, so we need to reparametrize the multivariate Gaussian $\\mathcal{GP}(\\mu, K_{\\sigma,l}(x_n))$ in order to ensure computational efficiency. Specifically, we model the logit probability as \n",
    "\n",
    "$$\\text{logit}(\\mathbf{p}) = \\mu \\cdot \\mathbf{1}_N + \\eta \\cdot L,$$\n",
    "\n",
    "where $L$ is the Cholesky factor of $K_{\\sigma,l}$ and $\\eta_n\\sim\\mathcal{N}(0,1)$. This relationship is implemented by the `get_logits` function below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(x, mu, sigma, l, eta):\n",
    "    kernel = gp.kernels.RBF(input_dim=2, variance=torch.tensor(sigma), \n",
    "                            lengthscale=torch.tensor(l))\n",
    "    K = kernel.forward(x, x) + torch.eye(x.shape[0]) * 1e-6\n",
    "    L = K.cholesky()\n",
    "    return mu+torch.mv(L,eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write a pyro model `gp_classifier(x,y)` that implements the reparametrized generative model, using `get_logits` function and `pyro.plate` on independent observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use pyro `NUTS` on the `gp_classifier` model to infer the posterior distribution of its parameters. Set `num_samples=10` and `warmup_steps=50`. Then extract the posterior samples using pyro `.get_samples()` and print the keys of this dictionary using `.keys()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `posterior_predictive` function below outputs the prediction corresponding to the $i$-th sample from the posterior distribution. `plot_pyro_predictions` calls this method to compute the average prediction on each input point and plots the posterior predictive mean function over a finite grid of points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_predictive(samples, i, x, x_grid):\n",
    "    kernel = gp.kernels.RBF(input_dim=2, variance=samples['sigma'][i], \n",
    "                            lengthscale=samples['l'][i]) \n",
    "    N_grid = x_grid.shape[0]\n",
    "    \n",
    "    y = get_logits(x, samples['mu'][i], samples['sigma'][i], \n",
    "                   samples['l'][i],  samples['eta'][i])\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        gpr = gp.models.GPRegression(x, y, kernel=kernel)\n",
    "        mean, cov = gpr(x_grid, full_cov=True)\n",
    "        \n",
    "    yhat = dist.MultivariateNormal(mean, cov + torch.eye(N_grid) * 1e-6).sample()    \n",
    "    return yhat.sigmoid().numpy()\n",
    "\n",
    "def plot_pyro_predictions(posterior_samples, x):\n",
    "\n",
    "    n_samples = posterior_samples['sigma'].shape[0]\n",
    "    x0, x1, x_grid = meshgrid(x, 30) \n",
    "    x_grid = torch.from_numpy(x_grid)\n",
    "    preds = np.stack([posterior_predictive(posterior_samples, i, x, x_grid) \n",
    "                      for i in range(n_samples)])\n",
    "\n",
    "    plt.figure(figsize=np.array([10, 6]))\n",
    "    plt.contourf(x0, x1, preds.mean(0).reshape(x0.shape), 101, \n",
    "                 cmap=plt.get_cmap('bwr'), vmin=0, vmax=1)\n",
    "    plt.title(f'Posterior Mean')\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "    plt.colorbar()\n",
    "    scatterplot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Pass the learned posterior samples obtained from NUTS inference to `plot_pyro_predictions` and plot the posterior predictive mean.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] [sklearn GP classifier](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html)\n",
    "\n",
    "[2] [pyro GPs](https://pyro.ai/examples/gp.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
